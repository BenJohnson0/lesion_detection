{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HCAIM CA3 - Human-Centric Deep Learning, Society & AI, Future of AI\n",
        "Ben Johnson X00229603, Ravindra Sai Cherukuru X00229508, Rohith Budugarla X00229544, Sravan Kumar Yannam X00229546.\n",
        "\n",
        "This research is in the domains of Medical Artificial Intelligence, Deep\n",
        "Learning, Explainable Artificial Intelligence (XAI) & Medical Image Classification. The focus is on developing deep learning models for automated medical image classification (of lesions from CT scans) while ensuring trust, transparency & fairness using XAI techniques. The study is relevant to the fields of healthcare, computational medicine, and responsible AI\n",
        "implementation in healthcare.\n",
        "\n",
        "In this project, we used some data from the NIH Deep Lesion Dataset, and some data from Mendeley's \"Abdominal CT scans\" dataset. We collected the data in this way so that we could build a CNN to classify lesions within CT scans. We used various XAI tools throughout to maintain explainability and transparecy, such as Saliency Maps, and LIME.\n",
        "\n",
        "Project Rationale: We chose to do this project in this field as it gives us a lot of scope to talk about compliance, regulation & legislation; We plan to use various XAI techniques to verify and support our findings, which should be easier to do as the dataset is related to such a sensitive topic. Additionally, we can discuss the EU AI Act, ethical concerns, data privacy & maintenance. Furthermore, we will conduct a comprehensive literature review in order to\n",
        "discover the current trends, emerging tools and future aims of the sector. Following the literature review, we will develop & compare some machine learning techniques such as K-Nearest Neighbours and Logistic Regression, which will be helpful as a baseline when we come to the design and implementation of a neural network, which will also be evaluated both in terms of initial development and XAI.\n",
        "\n",
        "Datasets:\n",
        "*   https://data.mendeley.com/datasets/6x684vg2bg/1\n",
        "*   https://www.kaggle.com/datasets/kmader/nih-deeplesion-subset/data\n",
        "\n"
      ],
      "metadata": {
        "id": "j1pdmSOnGv5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "qdcPu6PaJSGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-keras-vis lime tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "Cgy2Ea06Kje6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dzgUye39KmM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Exploration"
      ],
      "metadata": {
        "id": "TsZ-HHK2JYaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-processing libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "from PIL import ImageOps, Image\n",
        "import glob\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, BatchNormalization\n",
        "from tensorflow.keras import layers, Model, applications\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "from tf_keras_vis.utils import normalize\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n",
        "from tf_keras_vis.scorecam import Scorecam\n",
        "from tf_keras_vis.activation_maximization import ActivationMaximization\n",
        "from tf_keras_vis.activation_maximization.callbacks import Progress\n",
        "from tf_keras_vis.activation_maximization.input_modifiers import Jitter,Rotate2D\n",
        "from tf_keras_vis.activation_maximization.regularizers import TotalVariation2D, Norm\n",
        "from tf_keras_vis.utils.model_modifiers import ExtractIntermediateLayer, ReplaceToLinear\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "import lime\n",
        "from lime import lime_image\n",
        "\n",
        "# figure sizes\n",
        "plt.rcParams['figure.figsize'] = [5,5]\n",
        "num_classes = 2 # classification classes\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "_a97TGGWiP8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 1\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "6jJQTANqnRXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# append images into lists (class1 - Abdomen CT scans with Lesions)\n",
        "class1 = [Image.open(filename) for filename in glob.glob('/content/drive/MyDrive/ca3_data/abdomen_with_lesions/*.jpg')]\n",
        "\n",
        "# load class0 (Abdomen CT scans without Lesions, original size 400+ images)\n",
        "class0_files = glob.glob('/content/drive/MyDrive/ca3_data/abdomen_without_lesions/*.jpg')\n",
        "\n",
        "# randomly select 146 images from class0\n",
        "class0 = [Image.open(filename) for filename in random.sample(class0_files, 146)]"
      ],
      "metadata": {
        "id": "5_rLCwzlrf9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 146 images in each class. The low number is due to the fact that we combined datasets, and had to cut down to match, while also providing some validation data. We understand that this amount of data is not appropriate, but we had to continue with what we have."
      ],
      "metadata": {
        "id": "8-bw_vy14vcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Class0 size:\", len(class0))\n",
        "print(\"Class1 size:\", len(class1))"
      ],
      "metadata": {
        "id": "1X5724oarf7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Image Size"
      ],
      "metadata": {
        "id": "CFt0CcOlOHK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for displaying first 3 images\n",
        "def display_images(list):\n",
        "  for i in range(3):\n",
        "    display(list[i])"
      ],
      "metadata": {
        "id": "71PL86p3qrj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we checked the size of the first image in each dataset to help us understand what we were working with. Each image is 512x512px in size, they are too large for the type of CNN we want to develop, given the timeframe and resources available, it will need to be resized.\n",
        "\n",
        "We did run the code in full with images resized from 64x64 up to 512x512, and 128x128 proved to be the most suitable, as it never ran out of compute time and also maintained the most quality."
      ],
      "metadata": {
        "id": "M_WFei7gaSNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_first_image_size(image_list):\n",
        "  if len(image_list) > 0:\n",
        "    return image_list[0].size\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "print(\"First Abdomen (without Lesion) image size:\", get_first_image_size(class0))\n",
        "print(\"First Abdomen (with Lesion) image size:\", get_first_image_size(class1))"
      ],
      "metadata": {
        "id": "H3p-6KX2tnYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 First three images"
      ],
      "metadata": {
        "id": "cU-EC36IONQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first 3 abdomen without lesions\n",
        "display_images(class0)"
      ],
      "metadata": {
        "id": "DcvPCVWwsPJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first 3 abdomen with lesions\n",
        "display_images(class1)"
      ],
      "metadata": {
        "id": "uy6ylamlsShi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analysing the first three images in each dataset, and checking their sizes, we can clearly see that there is some preprocessing that needs to be done. The images are too large, and too high quality to be run in our proposed CNN and environment. The images need to be resized, scaled and recoloured / have contrast increased in order to develop a suitable, (hopefully!) accurate model.\n",
        "\n",
        "At this stage we have some concerns over the correct identification of lesions, as we cannot see where exactly the lesions are. We have kept this is mind further on in the development."
      ],
      "metadata": {
        "id": "QI2PNtRwaaS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Average of all images"
      ],
      "metadata": {
        "id": "Ph7dAeHTORik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure class0 and class1 are numpy arrays\n",
        "class0_copy = np.array(class0)\n",
        "class1_copy = np.array(class1)\n",
        "\n",
        "# calculate the mean image\n",
        "mean_image = np.mean(np.concatenate((class0_copy, class1_copy), axis=0), axis=0)\n",
        "\n",
        "# display the mean image\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(mean_image, cmap=cm.gray)\n",
        "plt.axis('off')\n",
        "plt.title('Average of All Abdomen Scans')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7MvapXizILcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We knew that CT Scans were presented as black images, with the scanned organ / body part appearing generally in the centre, as a white or grey shape. This average image is just a quick sanity check."
      ],
      "metadata": {
        "id": "hZoqQ4cZIb9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Image edge detection"
      ],
      "metadata": {
        "id": "9MhCzD98OVH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_edges(image_list, x, y):\n",
        "  img = image_list[0]\n",
        "  img = img.astype(np.uint8)\n",
        "\n",
        "  edges = cv2.Canny(img, x, y)\n",
        "\n",
        "  plt.subplot(121)\n",
        "  plt.imshow(img, cmap='gray')\n",
        "  plt.title('Original Image')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.subplot(122)\n",
        "  plt.imshow(edges, cmap='gray')\n",
        "  plt.title('Edge Image')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "define_edges(class0_copy, 100, 200)"
      ],
      "metadata": {
        "id": "OfniIig9IbOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "define_edges(class1_copy, 5, 10)"
      ],
      "metadata": {
        "id": "7gCyDz3UIfP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the Canny edge detection algorithm (thresholds set at 100, 200 for class0 and 5, 10 for class1) highlights sharp changes in intensity, typically corresponding to edges or contours in the image. It is not ideal that we have to use different intensities just to see the main part of the image, as this could become a problem later in development."
      ],
      "metadata": {
        "id": "_YyUXNGkKVA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Ethical Concerns\n",
        "To assess **compliance** with legislation like the EU AI Act and GDPR, we examined the dataset and model for risks in fairness & explainability. A major risk is bias, as the datasets are different, and as you have seen, each class looks very different, additiionally, the data may lack demographic diversity, leading to unequal model performance. Explainability is another key requirement for high-risk AI (our lesion classifier is high risk as per the EU AI Act); CNNs are often black-box models, so tools like GradCAM are going to be used to make decisions more interpretable for clinicians and patients alike. Finally, to support transparency, we ensured all data sources, preprocessing steps, and model training processes are well-documented and covered in the accompanying paper. These steps help mitigate risk and align the system with ethical and legal standards.\n",
        "\n",
        "Integrating AI ethics frameworks, like the EU AI Act, into the development of a CNN for abdominal lesion detection creates a foundation for evaluating and mitigating **ethical concerns** throughout the project. As mentioned previously, the EU AI Act categorizes medical AI systems as high-risk, which means they are subject to stricter regulatory requirements, including transparency, accountability, data governance, and human oversight. To follow the official guidelines, the use of the NIH DeepLesion and Mendeley Abdomen CT datasets required safe data handling, ensuring datasets were collected and shared ethically, with appropriate consent and anonymization. Furthermore, the Act emphasizes robustness and accuracy, requiring that models are trained and tested on diverse, representative data to minimize bias and ensure generalizable performance across different populations. Throughout this project, we have made sure that the data is anonymized and collected from trusted sources, provided in the first text block. Although these datasets are publicly available, there remains a risk of re-identification if combined with other data sources, a challenge that had to be actively managed.\n",
        "\n",
        "The Act also aligns with broader AI ethics principles promoted by the European Commission's High-Level Expert Group on AI, which demand respect for human autonomy, prevention of harm, fairness, and explainability. These principles challenged us to go beyond technical performance of models, and to consider how the model's development & deployment might affect workflows, trust, and patient-clinician relationships. For example, explainability becomes a legal and ethical obligation, not just a design feature, it is particularly important when radiologists must interpret and act on model outputs. Additionally, human oversight must be built into any further development of the system, ensuring that clinicians maintain final decision-making authority, thus preserving professional accountability and ethical responsibility.\n",
        "\n",
        "**Maintaining compliance** while leveraging AI in a medical imaging project like ours presents several practical challenges. For example, we had to balance model performance with explainability, which is a common trade-off in CNN architectures, as they are often black-box models. Meeting documentation and traceability requirements, such as logging tests, hyperparameter choices, and evaluation results are time-consuming but necessary under the EU AI Act's conformity assessments. We have made recordings of our meetings and have kept eachother constantly informed of our work. Moreover, the dynamic nature of medical data broadens the scope to model retraining over time, raising issues of continuous validation and monitoring that may need to be addressed through newer governance frameworks.\n",
        "\n",
        "The **main risks** associated with this system include data privacy breaches, biased predictions, lack of transparency, and over-reliance on automated outputs. These risks could compromise compliance with current regulations, such as the EU AI Act, the GDPR, and medical device regulations. For example, a misdiagnosis caused by a biased model could trigger legal liability and reputational harm, especially if it can be shown that insufficient diversity or validation was built into the training data. To mitigate these risks, it is essential that human oversight remains central in any future model development, with clinicians retaining final decision-making authority. Our data is generally poor, and we did not want to spend time augmenting medical data, as CT scans are always exact and precise. Building detailed documentation, bias mitigation strategies, and model explainability mechanisms into the development lifecycle will be critical to ensuring long-term ethical and legal compliance. We have tried to aid in this by creating LIME and GradCAM visualizations of the predictions."
      ],
      "metadata": {
        "id": "RjNUBhg7yh5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Potential Data Bias\n",
        "\n",
        "While training a CNN to classify lesions using the Mendeley Abdominal CT Scans and NIH DeepLesion datasets, it is important to address potential biases that may affect model generalizability. The Mendeley dataset, focused on stomach cancer detection, contains axial CT scans from a single hospital in Iran, which risks geographic and demographic bias (like limited representation of age, sex, or ethnic diversity). Similarly, while the NIH DeepLesion dataset includes over 32,000 annotated lesions across diverse regions (bone, liver, kidney, etc.), its reliance on data from a single American institution raises concerns about data sourcing (for example, differences in CT protocols / scanner differences) and underrepresentation of rare lesion types or patient subgroups. Additionally, as we will be using a small subset of the data, we risk introducing sampling bias."
      ],
      "metadata": {
        "id": "ebWkqEhUN_4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Data Preprocessing"
      ],
      "metadata": {
        "id": "U_pb0OcsJ_Rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `preprocess_and_save_images()` takes a list of PIL.Image objects, folder location and size, and applies a loop of:\n",
        "\n",
        "*   Resizing images to 128x128\n",
        "*   Converting to greyscale\n",
        "*   Contrast enhancement by redistributing pixel intensities (`.equalize()`)\n",
        "*   Normalization of pixel values.\n",
        "*   Saves the processed images as `.png` with filename `image_i.png`.\n",
        "\n",
        "The images are saved into \"clean\" class folders and later used in model training and testing. We did test images in 64x64 up to 512x512 pixel size, but the image quality was too blurry, or we ran out of resources, which is obviously detrimental to sensitive medical data like ours.\n",
        "\n"
      ],
      "metadata": {
        "id": "0lDBl04nbF7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('abdomen_with_lesion_preprocessed', exist_ok=True)\n",
        "os.makedirs('abdomen_without_lesion_preprocessed', exist_ok=True)\n",
        "\n",
        "def preprocess_and_save_images(image_list, folder_name, size=(128, 128)):\n",
        "    processed = []\n",
        "    for i, img in enumerate(image_list):\n",
        "        img = img.resize(size)\n",
        "        img = img.convert('L')\n",
        "        img = ImageOps.equalize(img)\n",
        "        img_arr = np.array(img) / 255.0\n",
        "        processed.append(img_arr)\n",
        "\n",
        "        # save the preprocessed images\n",
        "        save_path = os.path.join(folder_name, f'image_{i}.png')\n",
        "        Image.fromarray((img_arr * 255).astype(np.uint8)).save(save_path)\n",
        "    return processed\n",
        "\n",
        "# preprocess and save \"cleaned\" images\n",
        "class0_clean = preprocess_and_save_images(class0, 'abdomen_without_lesion_preprocessed')\n",
        "class1_clean = preprocess_and_save_images(class1, 'abdomen_with_lesion_preprocessed')"
      ],
      "metadata": {
        "id": "X6wG0zmahKt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_sample_images(class0_data, class1_data, num_samples=3):\n",
        "    plt.figure(figsize=(num_samples * 2, 4))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # class0 - Without Lesions\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        plt.imshow(class0_data[i], cmap='gray')\n",
        "        plt.title(\"No Lesion\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # class1 - With Lesions\n",
        "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
        "        plt.imshow(class1_data[i], cmap='gray')\n",
        "        plt.title(\"Lesion\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# display 4 samples from each class\n",
        "show_sample_images(class0_clean, class1_clean, num_samples=4)"
      ],
      "metadata": {
        "id": "hnCh21j44S0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, we have 4 images of each class, preprocessed and ready to be used in model development. We had some concerns at this point about the quality of the images, and we did test multiple processing pipelines (w/o equalization, w/o normalization for example). It was difficult to work with the data due to the use of two different datasets."
      ],
      "metadata": {
        "id": "gJtcLvH4cn7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8 Data Splitting\n",
        "At the end of this section, we are left with:\n",
        "```\n",
        "X_train shape: (204, 128, 128, 1)\n",
        "y_train shape: (204,)\n",
        "X_test shape: (44, 128, 128, 1)\n",
        "y_test shape: (44,)\n",
        "```\n",
        "with some additional validation data that is used later on within a deep learning model."
      ],
      "metadata": {
        "id": "EefxZ4_H4d56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# combine data\n",
        "X = np.array(class0_clean + class1_clean).reshape(-1, 128, 128, 1)\n",
        "y = np.array([0]*len(class0_clean) + [1]*len(class1_clean))"
      ],
      "metadata": {
        "id": "FaiK4Lwt4lza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# perform train / test split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42) # temp values required for validation data\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42) # validation data required for later models"
      ],
      "metadata": {
        "id": "pU5ONWXt4qrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "YdMvVERk4teg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Development\n",
        "In this section, we developed 4 Deep Learning models. Two Convolutional Neural Networks, One CNN with Transfer Learning and attempted a model that uses pruning. In the previous step, we can see that the data is split nicely into training and testing sets of 204 images and 44 images respectively, and 128x128 in size. We also developed 2 \"traditional\" machine learning models and compared their performance to our chosen deep learning model."
      ],
      "metadata": {
        "id": "LItBV0lSLbVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Kernel Size Investigation\n",
        "\n",
        "Firstly, we wanted to test a few kernel sizes in order to find a suitable structure for our final model."
      ],
      "metadata": {
        "id": "RZ9UCRRCM8Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w, h = 128, 128"
      ],
      "metadata": {
        "id": "rwuGAnvthB88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernelSizes = [3,5,7]\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "for ks in kernelSizes:\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (ks, ks), strides=1, padding=\"valid\", input_shape=(w, h, 1), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding=\"valid\"))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "  # fit the model\n",
        "  history = model.fit(X_train, to_categorical(y_train), validation_split=0.2, epochs=15, batch_size=32, verbose=0)\n",
        "\n",
        "  print(\"kernel Size:\", ks)\n",
        "\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper right')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "zmrj1DRrM8wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Batch Size Investigation\n",
        "\n",
        "Secondly, we tested some batch sizes to find a suitable size."
      ],
      "metadata": {
        "id": "dem3pAeuM9Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batchSizes = [16, 24, 32, 64, 128]\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "for bs in batchSizes:\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), strides=1, padding=\"valid\", input_shape=(w, h, 1), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding=\"valid\"))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "  # fit the model\n",
        "  history = model.fit(X_train, to_categorical(y_train), validation_split=0.2, epochs=15, batch_size=bs, verbose=0)\n",
        "\n",
        "  print(\"Batch Size:\", bs)\n",
        "\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper right')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "zcXe_DOVNHeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Model Performance Evaluation\n",
        "\n",
        "From our kernel size and batch size testing, we can conclude that:\n",
        "*   Batch sizes equal to and less than 24, and batch sizes greater than 64 are not suitable or offer diminishing returns (as shown after epoch 8 in batch sizes 64 & 128). Due to this test, we will proceed with batch sizes 24 and 32 in our models, eventually settling on a final choice after other parameters are chosen.\n",
        "*   Kernel size of 3x3 seems to be the obvious choice, it displays a smooth learning curve when compared to 5x5 and 7x7, and as it is smaller, it can capture finer details of our images, an important factor for CT scans."
      ],
      "metadata": {
        "id": "sgPVI27DLhzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "From the previous step, we discovered that using:\n",
        "*   `Kernel size [3x3]`\n",
        "*   `Batch size [24 or 32]`\n",
        "\n",
        "were the most suitable parameters for training. In the next section, we imported early stopping so that we do not have to worry as much about how many epochs we use, only to stop training once the model is not improving. We also added dropout for testing and a function to create a confusion matrix."
      ],
      "metadata": {
        "id": "HYCV972HPKTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# dropout\n",
        "from tensorflow.keras.layers import Dropout"
      ],
      "metadata": {
        "id": "u-RWQ4YbLq4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(model, X, Y, class_labels=None, title='Confusion Matrix'):\n",
        "\n",
        "  # get predictions and true labels\n",
        "  Y_pred = model.predict(X)\n",
        "  Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "  Y_true = np.argmax(Y, axis=1)\n",
        "\n",
        "  # compute confusion matrix\n",
        "  conf_matrix = confusion_matrix(Y_true, Y_pred_classes)\n",
        "\n",
        "  if class_labels is None:\n",
        "    num_classes = conf_matrix.shape[0]\n",
        "    class_labels = [f'Class {i}' for i in range(num_classes)]\n",
        "\n",
        "  # plot\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "  xticklabels=class_labels, yticklabels=class_labels)\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('True')\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "rANmGlAePlrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_class = np.argmax(y_pred, axis=1) # get predicted class labels (0 or 1)\n",
        "\n",
        "    if y_test.ndim == 2 and y_test.shape[1] > 1:\n",
        "        # if y_test is one-hot encoded, convert it back to original labels\n",
        "        y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_class).ravel()\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    print(f\"Sensitivity: {sensitivity:.2%}\")\n",
        "    print(f\"Specificity: {specificity:.2%}\")\n",
        "\n",
        "    return sensitivity, specificity"
      ],
      "metadata": {
        "id": "qqZXgx3Fje99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model A"
      ],
      "metadata": {
        "id": "jLFKaqkTQV5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelA = Sequential([\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.2),\n",
        "    Flatten(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "modelA.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train Model A\n",
        "historyA = modelA.fit(X_train, to_categorical(y_train), validation_data=(X_test, to_categorical(y_test)), epochs=30, batch_size=24, verbose=1, callbacks=early_stopping)\n",
        "\n",
        "# plot accuracy\n",
        "plt.plot(historyA.history['accuracy'], label='accuracy')\n",
        "plt.plot(historyA.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# plot loss\n",
        "plt.plot(historyA.history['loss'], label='train')\n",
        "plt.plot(historyA.history['val_loss'], label='val')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3DQ7psHAQZLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(modelA, X_train, to_categorical(y_train), class_labels=['with Lesion', 'without Lesion'], title='Confusion Matrix for Model A')"
      ],
      "metadata": {
        "id": "WMRjdp3LS7xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(modelA, X_test, to_categorical(y_test))"
      ],
      "metadata": {
        "id": "y_qsfCnBjhFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model B"
      ],
      "metadata": {
        "id": "FBh9bvlJQZc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelB = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "modelB.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "historyB = modelB.fit(X_train, to_categorical(y_train), validation_data=(X_test, to_categorical(y_test)), epochs=30, batch_size=32, verbose=1, callbacks=early_stopping)\n",
        "\n",
        "# plot accuracy\n",
        "plt.plot(historyB.history['accuracy'], label='accuracy')\n",
        "plt.plot(historyB.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# plot loss\n",
        "plt.plot(historyB.history['loss'], label='train')\n",
        "plt.plot(historyB.history['val_loss'], label='val')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EiMwRJ1oQgqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(modelB, X_train, to_categorical(y_train), class_labels=['with Lesion', 'without Lesion'], title='Confusion Matrix for Model B')"
      ],
      "metadata": {
        "id": "AHokgNOUS8OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(modelB, X_test, to_categorical(y_test))"
      ],
      "metadata": {
        "id": "tqh89LPZqR_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Emerging Tools Implementation"
      ],
      "metadata": {
        "id": "w9hBSf90QaS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.1 Model C - Transfer Learning using MobileNetV2\n",
        "\n",
        "Using MobileNetV2 for our CNN may not be suitable because the model is pre-trained on ImageNet, which expects RGB images of size 224x224, while our CT scan images are greyscale and 128x128. Converting greyscale images to RGB by duplicating channels and resizing them to a larger resolution may introduce unnecessary computational overhead and risk losing important spatial and intensity details specific to medical imaging. Moreover, MobileNetV2's architecture is optimized for natural images, not medical scans, which could lead to suboptimal feature extraction and reduced model performance. A custom or medical-imaging-specific CNN architecture may therefore be more appropriate for this context."
      ],
      "metadata": {
        "id": "DUmGTL12shoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (224, 224, 3)"
      ],
      "metadata": {
        "id": "7tjPpTXc0hxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_and_add_channels(images, target_shape=(224, 224, 3)):\n",
        "  resized_images = []\n",
        "  for img in images:\n",
        "    # resize images using cv2\n",
        "    resized_img = cv2.resize(img, target_shape[:2])\n",
        "\n",
        "    # if the image is greyscale, it is converted to RGB by stacking 3 channels\n",
        "    if resized_img.ndim == 2:\n",
        "      resized_img = np.stack((resized_img,) * 3, axis=-1)\n",
        "\n",
        "    resized_images.append(resized_img)\n",
        "  return np.array(resized_images)"
      ],
      "metadata": {
        "id": "VsJWANNRsoey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resizing training and val data\n",
        "X_train_resized = resize_and_add_channels(X_train)\n",
        "X_val_resized = resize_and_add_channels(X_val)\n",
        "X_test_resized = resize_and_add_channels(X_test)"
      ],
      "metadata": {
        "id": "SZGCzT0xxAlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained base model (imagenet)\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "mMiU9fwTsocc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze base\n",
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "u8Rdnrz5soaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add custom head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)"
      ],
      "metadata": {
        "id": "k_Nvoc39s71w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_transfer = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "Mv7WFa8Ts7zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_transfer.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RkubqFrBs7xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_transfer = model_transfer.fit(\n",
        "    X_train_resized, to_categorical(y_train),\n",
        "    validation_data=(X_val_resized, to_categorical(y_val)),\n",
        "    epochs=10,\n",
        "    batch_size=3\n",
        ")"
      ],
      "metadata": {
        "id": "FUNUonJfuvZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model_transfer.evaluate(X_test_resized, to_categorical(y_test))\n",
        "print(f\"Transfer Model Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "JREnnuJPuxLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_base, test_acc_base = modelB.evaluate(X_test, to_categorical(y_test))\n",
        "print(f\"Baseline CNN Test Accuracy: {test_acc_base:.4f}\")"
      ],
      "metadata": {
        "id": "v9ss6oaauxIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# measuring time for a batch of predictions\n",
        "start_time = time.time()\n",
        "preds = model_transfer.predict(X_test_resized)\n",
        "end_time = time.time()\n",
        "transfer_infer_time = (end_time - start_time) / len(X_test_resized)\n",
        "print(f\"Transfer Learning Inference Time per image: {transfer_infer_time:.5f} seconds\")"
      ],
      "metadata": {
        "id": "_7mkyqnouxFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat for baseline modelB\n",
        "start_time = time.time()\n",
        "preds_base = modelB.predict(X_test)\n",
        "end_time = time.time()\n",
        "base_infer_time = (end_time - start_time) / len(X_test)\n",
        "print(f\"Baseline CNN Inference Time per image: {base_infer_time:.5f} seconds\")"
      ],
      "metadata": {
        "id": "XRlGsXFNuxCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_transfer.save(\"transfer_model.h5\")\n",
        "modelB.save(\"baseline_model.h5\")"
      ],
      "metadata": {
        "id": "WdGBKtvwuw_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transfer_size = os.path.getsize(\"transfer_model.h5\") / (1024 * 1024)\n",
        "baseline_size = os.path.getsize(\"baseline_model.h5\") / (1024 * 1024)\n",
        "\n",
        "print(f\"Transfer Model Size: {transfer_size:.2f} MB\")\n",
        "print(f\"Baseline Model Size: {baseline_size:.2f} MB\")"
      ],
      "metadata": {
        "id": "H_0ytrvyu9wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results reveal a subtle comparison between the Baseline CNN and the Transfer Learning model.\n",
        "*   Both models achieved a perfect test accuracy of 100%, indicating either excellent performance on the classification task, or some kind of overfitting problem, which we believe is the case.\n",
        "*   Closer analysis of other metrics reveals key differences. The Transfer Learning model, despite its smaller size (10MB~ vs. 20MB~), demonstrated a ~6-7% higher inference time per image compared to the Baseline CNN. This suggests that while transfer learning has the potential to produce highly accurate and compact models, the computational cost during inference can be higher, potentially due to the complexity of the pre-trained architecture, and the quality / size of our data.\n",
        "*   The loss values also differed, with the Transfer Learning model showing a sligtly higher loss than the Baseline CNN, although both values remain relatively low.\n",
        "\n",
        "These findings suggest that while both approaches are viable, the choice between a baseline and a transfer learning model should consider the trade-offs between model size, inference time, loss metric and data type."
      ],
      "metadata": {
        "id": "_acmFRbUVVmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.2 Model D - Model Compression using Pruning\n",
        "\n",
        "Unfortunately, we could not make model compression using pruning work for us. We tried the official Google Colab provided pruning file, notes and files from Rajesh's module, Gemini and others to no end. We do understand how pruning would work with our CNN however.\n",
        "\n",
        "1. Using magnitude-based pruning to gradually remove low-importance weights during training.\n",
        "\n",
        "2. Set a sparsity range (possibily 0% to 50%) across specific epochs to avoid sudden accuracy loss.\n",
        "\n",
        "3. Wrap the full model or selected layers using a pruning API..\n",
        "\n",
        "4. Retrain or fine-tune while pruning is applied, allowing the optimizer to focus on the remaining weights.\n",
        "\n",
        "5. Evaluate metrics throughout training to ensure pruning doesn't degrade accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "yTaomQiCtO0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model architecture.\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(128, 128)),\n",
        "  keras.layers.Reshape(target_shape=(128, 128, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(2)\n",
        "])\n",
        "\n",
        "# train classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "  X_train,\n",
        "  y_train,\n",
        "  epochs=4,\n",
        "  validation_split=0.1,\n",
        ")"
      ],
      "metadata": {
        "id": "-uVBCwcgbF0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot"
      ],
      "metadata": {
        "id": "7P6QnDy_HKEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define pruning\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude"
      ],
      "metadata": {
        "id": "qWxTRPYlJI8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation"
      ],
      "metadata": {
        "id": "1iZUGbHpbeUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = X_train.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs"
      ],
      "metadata": {
        "id": "LVWKevPbbgdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                              final_sparsity=0.80,\n",
        "                                                              begin_step=0,\n",
        "                                                              end_step=end_step)\n",
        "}"
      ],
      "metadata": {
        "id": "1fvgEfZMHKCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.layers:  # iterate layers\n",
        "    if isinstance(layer, (keras.layers.Conv2D, keras.layers.Dense)):  # check layer type & prune\n",
        "        pruned_layer = prune_low_magnitude(layer, **pruning_params)"
      ],
      "metadata": {
        "id": "HP4mlhifcei2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_pruning = model"
      ],
      "metadata": {
        "id": "b8I1x9VZcisy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recompile after prune\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DRs1M_aSHJ_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
      ],
      "metadata": {
        "id": "EwGrf1GHXObP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "id": "_XBS6lKPHj_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Deep Learning Model comparison"
      ],
      "metadata": {
        "id": "XE4lPbxJxtvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot comparison\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(historyA.history['val_accuracy'], label='Model A')\n",
        "plt.plot(historyB.history['val_accuracy'], label='Model B')\n",
        "plt.plot(history_transfer.history['val_accuracy'], label='Model C - Transfer Learning')\n",
        "plt.title('Validation & Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6zb1GTf3xyin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Traditional ML Models\n",
        "\n",
        "This section will contrast modelB's performance with a standard Logistic Regression model and a K-Nearest Neighbours model."
      ],
      "metadata": {
        "id": "5A3RmF18mTwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.1 Logistic Regression"
      ],
      "metadata": {
        "id": "CsV_oqrq8tCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['No Lesion', 'Lesion']"
      ],
      "metadata": {
        "id": "ek-WwesjQvZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cm(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "H49G-u2em9gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "X_test_flat = X_test.reshape((X_test.shape[0], -1))"
      ],
      "metadata": {
        "id": "sL-YL-_AmfZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled = scaler.transform(X_test_flat)"
      ],
      "metadata": {
        "id": "ccHFM0KXmfTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "log_reg_preds = log_reg.predict(X_test_scaled)\n",
        "\n",
        "print(\"Logistic Regression Report:\")\n",
        "print(classification_report(y_test, log_reg_preds))\n",
        "\n",
        "plot_cm(y_test, log_reg_preds, \"Confusion Matrix: Logistic Regression\")"
      ],
      "metadata": {
        "id": "HbQnXDpImfPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.2 K-Nearest Neighbours"
      ],
      "metadata": {
        "id": "H7U0O7J_81f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "knn_preds = knn.predict(X_test_scaled)\n",
        "\n",
        "print(\"KNN Report:\")\n",
        "print(classification_report(y_test, knn_preds))\n",
        "\n",
        "plot_cm(y_test, knn_preds, \"Confusion Matrix: KNN\")"
      ],
      "metadata": {
        "id": "iB-CBIwKm5Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.3 Comparison\n",
        "\n",
        "Three models were evaluated on a balanced test set of 44 abdominal CT scans (22 lesion/22 no-lesion). Logistic Regression achieved perfect classification (100% accuracy, precision, recall, F1), while KNN scored 98% accuracy with slight recall drops for lesions (95%). The CNN (modelB) reported 100% test accuracy and 100% sensitivity/specificity. While these metrics suggest exceptional performance, they would never be used for medical AI, where even state-of-the-art classifiers rarely exceed 80% - 90% accuracy on diverse datasets, where humans are always given the final call. The high scores across different algorithms (linear, distance-based, deep learning) hints at systemic or data issues, rather than genuine model performance.The performance likely stems from dataset limitations and flaws:\n",
        "\n",
        "*   The small test size (only 44) creates a weakness, where a single misclassification alters accuracy by ~2%.\n",
        "*   The artificial 50-50 class balance fails to reflect clinical prevalence (for example, lesions may occur in only ~5% of real scans), inflating results.\n",
        "*   Preprocessing steps may have introduced undesirable patterns (such as intense contrast differences) that models have exploited as shortcuts, rather than learning true lesion location, shape, and texture.\n",
        "\n",
        "While the models technically achieved near perfect metrics, these likely reflect design and data limitations rather than clinical \"perfection\". In practice, lesion classification demands comprehensive validation on multi-institutional data with varied CT scanners and protocols, a benchmark where even a tiny accuracy drop could have significant implications. Finally, while promising, these results require cautious interpretation until broader validation is conducted and model pre-preprocessing is improved."
      ],
      "metadata": {
        "id": "p90noXdFY83c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Final Model Selection\n",
        "Both models performed really well, however we are wary of being too excited, as mentioned previously, it could be a really good model, or it could be an issue, overfitting, poor data. As we have used two different datasets, and tried to match them before training, we think that the model is not effectively classifying the lesions, rather overfitting or guessing.\n",
        "\n",
        "There is nothing we can do at this stage of the project so we will move on. The final model structure is shown below. It is made up of 2 convolutional layers with 3x3 kernel size and ReLU activation, sizes 32 and 64 respectively. After each layer, we have a 2x2 MaxPooling layer, followed by a 0.2 Dropout layer in an attempt to combat overfitting in Conv2D layer 1, and a Flatten layer at the end. Finally, we have a Dense(32) layer with ReLU activation and then a Dense output layer with 2 classes using softmax output.\n",
        "\n",
        "We used 30 epochs (with early stopping, patience 5) and batch size 32. These parameters were identified earlier.\n",
        "\n",
        "```\n",
        "modelB = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "modelB.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs=30\n",
        "batch_size=32\n",
        "callbacks=early_stopping\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_rUEea2LlsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Unseen data prediction"
      ],
      "metadata": {
        "id": "TFHsMj9-HGQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_unseen_data(image_list, folder_name, size=(128, 128)):\n",
        "    processed = []\n",
        "    for img in image_list:\n",
        "        original_name = os.path.basename(img.filename)  # keep original filename\n",
        "        img = img.resize(size)\n",
        "        img = img.convert('L')\n",
        "        img = ImageOps.equalize(img)\n",
        "        img_arr = np.array(img) / 255.0\n",
        "        processed.append(img_arr)\n",
        "\n",
        "        # save using original filename\n",
        "        save_path = os.path.join(folder_name, original_name)\n",
        "        Image.fromarray((img_arr * 255).astype(np.uint8)).save(save_path)\n",
        "    return processed"
      ],
      "metadata": {
        "id": "Ux1F5Qx-zzy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory to save preprocessed images\n",
        "os.makedirs('unseen_preprocessed', exist_ok=True)\n",
        "\n",
        "# load original unseen images\n",
        "unseen_paths = glob.glob('/content/drive/MyDrive/ca3_data/unseen_data/*.jpg')\n",
        "\n",
        "# open images using PIL\n",
        "unseen_images = [Image.open(path) for path in unseen_paths]\n",
        "\n",
        "# save preprocessed versions\n",
        "unseen_clean = preprocess_unseen_data(unseen_images, 'unseen_preprocessed')"
      ],
      "metadata": {
        "id": "xz2uH36SvHse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = modelB\n",
        "\n",
        "unseen_path = \"unseen_preprocessed/*.jpg\"\n",
        "\n",
        "# counters\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "true_labels = []\n",
        "pred_probs = []\n",
        "\n",
        "for filename in glob.glob(unseen_path):\n",
        "    im = Image.open(filename)\n",
        "\n",
        "    # display image\n",
        "    plt.imshow(im, cmap='gray')\n",
        "    plt.title(f\"Image: {os.path.basename(filename)}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # convert to numpy array and normalize\n",
        "    im_arr = np.array(im).reshape(1, w, h, 1).astype('float32') / 255.0\n",
        "\n",
        "    # predict probability\n",
        "    pred_prob = final_model.predict(im_arr, verbose=0)[0][0]\n",
        "    pred = 1 if pred_prob >= 0.5 else 0  # 0.5 threshold\n",
        "    pred_label = \"Lesion\" if pred == 1 else \"No Lesion\"\n",
        "    print(\"Predicted as:\", pred_label)\n",
        "\n",
        "    # get true label from filename\n",
        "    if \"lesion\" in filename.lower():\n",
        "        true = 1\n",
        "        true_label = \"Lesion\"\n",
        "    elif \"nolesion\" in filename.lower():\n",
        "        true = 0\n",
        "        true_label = \"No Lesion\"\n",
        "    else:\n",
        "        print(\"Filename does not contain label info. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    print(\"Actual label:\", true_label)\n",
        "\n",
        "    # store for RMSE\n",
        "    true_labels.append(true)\n",
        "    pred_probs.append(pred_prob)\n",
        "\n",
        "    if pred == true:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "if total > 0:\n",
        "    print(f\"\\nModel Accuracy on {total} Unseen Images: {(correct / total) * 100:.2f}%\")\n",
        "\n",
        "    # RMSE\n",
        "    rmse = np.sqrt(np.mean((np.array(true_labels) - np.array(pred_probs)) ** 2))\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "else:\n",
        "    print(\"\\nNo valid labeled images found.\")"
      ],
      "metadata": {
        "id": "9KJPrTSoHMY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation of our model on 10 unseen abdominal CT scan images resulted in an accuracy of just 40.00%, indicating that the model correctly classified only 4 out of 10 images. Additionally, the Root Mean Squared Error was 0.7746, which reflects a relatively high average deviation between the predicted and true class probabilities. These results suggest that the model struggles with generalization, potentially due to overfitting on the training data, limited dataset diversity, or weak decision boundaries. Further data augmentation, model tuning, or regularization techniques may be necessary to improve performance on unseen cases. The next section uses LIME and Saliency Maps to further explain the results."
      ],
      "metadata": {
        "id": "vtvbZblt_GvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Explainability (XAI)\n",
        "As XAI is a core part of the model that we developed, we chose to use Saliency Maps and LIME to explain why the model made the predictions it did. There are discussions on Saliency Map & LIME XAI in this section, as well as a Glassbox vs. Blackbox discussion in section 4.1."
      ],
      "metadata": {
        "id": "i4kWGbSFLlqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Saliency Maps"
      ],
      "metadata": {
        "id": "0hrHqBNTL_11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X =[]\n",
        "Y =[]\n",
        "\n",
        "for filename in glob.glob('/content/drive/MyDrive/ca3_data/abdomen_without_lesions/*.jpg'):\n",
        "  im=Image.open(filename).convert('L').resize((w,h),Image.LANCZOS)\n",
        "  arr = np.array(im)\n",
        "  X.append(arr)\n",
        "  Y.append(0) # no lesions class\n",
        "\n",
        "for filename in glob.glob('/content/drive/MyDrive/ca3_data/abdomen_with_lesions/*.jpg'):\n",
        "  im=Image.open(filename).convert('L').resize((w,h),Image.LANCZOS)\n",
        "  arr = np.array(im)\n",
        "  X.append(arr)\n",
        "  Y.append(1) # lesions class\n",
        "\n",
        "# convert to NP array\n",
        "X = np.array(X)\n",
        "\n",
        "# reshape to be [samples][channels][width][height]\n",
        "X = X.reshape(X.shape[0], w, h, 1).astype('float32')\n",
        "\n",
        "# normalize\n",
        "X = X /255\n",
        "\n",
        "# encode outputs\n",
        "Y = np.array(Y)\n",
        "\n",
        "# randomize the data set - numpy arrays\n",
        "randomize = np.arange(len(X))\n",
        "np.random.shuffle(randomize)\n",
        "X = X[randomize]\n",
        "Y = Y[randomize]\n",
        "\n",
        "Y = tf.keras.utils.to_categorical(Y)\n",
        "num_classes = Y.shape[1]"
      ],
      "metadata": {
        "id": "Wq-MxfqIT9Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_maps_of_a_layer(feature_maps):\n",
        "  plt.figure(figsize=(15,15))\n",
        "  plt.subplots(figsize=(20,20))\n",
        "  ix = 1\n",
        "  for _ in range(8):\n",
        "    for _ in range(4):\n",
        "      ax = plt.subplot(8, 4, ix)\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "      plt.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        "      ix += 1 # increase the index of the last dimension to visualize 8 feature maps\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A9JwhBrZVdPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_lesions_path = glob.glob('/content/drive/MyDrive/ca3_data/abdomen_with_lesions_test/*.jpg')\n",
        "without_lesions_path = glob.glob('/content/drive/MyDrive/ca3_data/abdomen_without_lesions_test/*.jpg')\n",
        "\n",
        "selected_paths = with_lesions_path + without_lesions_path\n",
        "\n",
        "Xt = []\n",
        "Xraw = []\n",
        "image_titles = []\n",
        "class_values = []\n",
        "\n",
        "for path in selected_paths:\n",
        "  img_rgb = Image.open(path).resize((w, h), Image.LANCZOS).convert(\"RGB\")\n",
        "  Xraw.append(img_rgb)\n",
        "\n",
        "  img_gray = img_rgb.convert('L')\n",
        "  Xt.append(np.array(img_gray))\n",
        "\n",
        "  if 'abdomen_with_lesions_test' in path:\n",
        "    image_titles.append(\"No Lesion\")\n",
        "    class_values.append(0)\n",
        "  else:\n",
        "    image_titles.append(\"Lesion\")\n",
        "    class_values.append(1)\n",
        "\n",
        "# convert image_titles to specific titles\n",
        "image_titles = [f\"{title} {i+1}\" for i, title in enumerate(image_titles)]\n",
        "\n",
        "Xt = np.array(Xt).reshape(len(Xt), w, h, 1).astype('float32') / 255"
      ],
      "metadata": {
        "id": "jhKdIljTWQcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = Model(inputs=modelB.inputs, outputs=modelB.layers[0].output)\n",
        "feature_maps_1 = model_1.predict(Xt)\n",
        "print('[*] feature_maps_1.shape: ', feature_maps_1.shape)\n",
        "plot_feature_maps_of_a_layer(feature_maps_1)"
      ],
      "metadata": {
        "id": "DDkb8_6iVdMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_vanilla_saliency_of_a_model(model, X, image_titles, class_values):\n",
        "  score = CategoricalScore(class_values)\n",
        "  saliency = Saliency(model, model_modifier=None, clone=True)\n",
        "  saliency_map = saliency(score, X)\n",
        "  f, ax = plt.subplots(nrows=2, ncols=5, figsize=(16, 6))\n",
        "  ax = ax.flatten()\n",
        "  for i, title in enumerate(image_titles):\n",
        "    ax[i].set_title(title, fontsize=14)\n",
        "    ax[i].imshow(saliency_map[i], cmap='jet')\n",
        "    ax[i].axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "bvMuH-P3WQWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_vanilla_saliency_of_a_model(modelB, Xt, image_titles, class_values)"
      ],
      "metadata": {
        "id": "AeEgdCn_XNjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_vanilla_saliency_overlay(model, X, image_titles, class_values, Xraw_RGB):\n",
        "  score = CategoricalScore(class_values)\n",
        "  saliency = Saliency(model, model_modifier=None, clone=True)\n",
        "  saliency_map = saliency(score, X)\n",
        "\n",
        "  f, ax = plt.subplots(nrows=2, ncols=5, figsize=(16, 6))\n",
        "  ax = ax.flatten()\n",
        "\n",
        "  for i, title in enumerate(image_titles):\n",
        "    # normalize saliency map\n",
        "    saliency_norm = (saliency_map[i] - saliency_map[i].min()) / (np.ptp(saliency_map[i]) + 1e-8)\n",
        "\n",
        "    # convert saliency to 3-channel heatmap\n",
        "    heatmap = cm.jet(saliency_norm)[..., :3]\n",
        "    heatmap = np.uint8(heatmap * 255)\n",
        "    heatmap_img = Image.fromarray(heatmap).resize((w, h), resample=Image.BILINEAR)\n",
        "\n",
        "    # resize and prepare base image\n",
        "    base_img = Xraw_RGB[i].resize((w, h)).convert(\"RGB\")\n",
        "\n",
        "    # overlay\n",
        "    base_np = np.array(base_img)\n",
        "    heat_np = np.array(heatmap_img)\n",
        "\n",
        "    blended = np.uint8(0.7 * heat_np + 0.3 * base_np)\n",
        "\n",
        "    ax[i].set_title(title, fontsize=14)\n",
        "    ax[i].imshow(blended)\n",
        "    ax[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IIo5ybgZWQRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_vanilla_saliency_overlay(modelB, Xt, image_titles, class_values, Xraw)"
      ],
      "metadata": {
        "id": "LvftWTc-Xb8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_gradcam_plusplus_of_a_model(model, X, image_titles, class_values, Xraw_RGB):\n",
        "  score = CategoricalScore(class_values)\n",
        "  gradcam = GradcamPlusPlus(model, model_modifier=None, clone=True)\n",
        "  cam = gradcam(score, X, penultimate_layer=-1)\n",
        "\n",
        "  f, ax = plt.subplots(nrows=2, ncols=5, figsize=(16, 6))\n",
        "  ax = ax.flatten()\n",
        "\n",
        "  for i, title in enumerate(image_titles):\n",
        "    cam_norm = (cam[i] - cam[i].min()) / (np.ptp(cam[i]) + 1e-8)\n",
        "    cam_norm = 1.0 - cam_norm\n",
        "\n",
        "    cam_resized = Image.fromarray(np.uint8(cam_norm * 255)).resize((w, h), resample=Image.BILINEAR)\n",
        "    cam_resized_np = np.array(cam_resized).astype(np.float32) / 255.0\n",
        "\n",
        "    heatmap = cm.jet(cam_resized_np)[..., :3]\n",
        "    heatmap = np.uint8(heatmap * 255)\n",
        "\n",
        "    # prepare base image\n",
        "    base_img = Xraw_RGB[i].resize((w, h)).convert(\"RGB\")\n",
        "    base_np = np.array(base_img)\n",
        "\n",
        "    # blend heatmap and image\n",
        "    blended = np.uint8(0.5 * heatmap + 0.5 * base_np)\n",
        "\n",
        "    ax[i].set_title(title, fontsize=14)\n",
        "    ax[i].imshow(blended)\n",
        "    ax[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9CmdIoXiXjLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_gradcam_plusplus_of_a_model(modelB, Xt, image_titles, class_values, Xraw)"
      ],
      "metadata": {
        "id": "I4kBpypKX-JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saliency Maps (XAI Discussion)\n",
        "\n",
        "Our saliency map results provide a direct visualization of which pixels most influence the model's predictions on unseen data by computing the gradient of the output with respect to the input image. Saliency maps are particularly valuable in medical imaging like ours, where focusing attention to specific regions is critical for clinical trust & explainability. In our visualizations, the lesion images on the bottom row show high activation outside the organ, and only some within the abdominal organs. This suggests that while our classifier is focusing on some relevant regions to make its decisions, it's main focus is on the irrelevant background. The highlighting implies that the model has learned only some of the internal representation of lesion features, rather than true prediction areas like:\n",
        "*   Texture irregularities\n",
        "*   Density anomalies\n",
        "*   Shape distortions within the organ boundaries\n",
        "*   Overall shape of the abdomen\n",
        "\n",
        "Similarly, the saliency maps for non-lesion images show a notable amount of activation outside the organs. These red-activated areas are found near the image borders or at the extreme edges of the images. We observed that this pattern may indicate that our model is overfitting to background noise or imaging features that are unrelated to lesion classification, probably due to the multiple datasets used. Instead of focusing on organ structures when determining that a scan is healthy, the model seems to be relying on features irrelevant to the abdomen itself. This type of drift is concerning, as it can lead to poor generalization, severe overfitting or failures in edge-case scenarios (as seen in our unseen images test).\n",
        "\n",
        "This misalignment is particularly meaningful because saliency maps, unlike LIME, do not rely on sampling or local approximations; they offer a direct gradient-based explanation of what drives the model's output. Therefore, when a saliency map highlights incorrect or irrelevant areas, it often indicates a deeper issue with the model's internal structure, in our case, we believe that it is the dataset and preprocessing. While the non-lesion maps show slightly less activation, the saliency maps for lesion cases highlight our model's inconsistent attention behavior across classes, which could be due to class imbalance, noise in the data (focus on the background), or insufficient class representation during model training."
      ],
      "metadata": {
        "id": "h8XVTJ771L9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 LIME"
      ],
      "metadata": {
        "id": "S2qYFcmgL_mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_labels = np.argmax(tf.keras.utils.to_categorical(y_test), axis=1)\n",
        "class_names = {0: 'No Lesion', 1: 'Lesion'}"
      ],
      "metadata": {
        "id": "KNwMIln_Tsg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_indices = {cls: np.where(y_test_labels == cls)[0][:3] for cls in [0, 1]}"
      ],
      "metadata": {
        "id": "IKGnhDDITsex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = lime_image.LimeImageExplainer()"
      ],
      "metadata": {
        "id": "gdpe08pPTscQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lime_predict_fn(images):\n",
        "  if images.ndim == 4 and images.shape[-1] == 3:\n",
        "    images = images.mean(axis=-1, keepdims=True)\n",
        "  return modelB.predict(images.reshape(-1, 128, 128, 1))"
      ],
      "metadata": {
        "id": "MDVx_if0TsZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(24, 12))"
      ],
      "metadata": {
        "id": "osTrNfjrT6K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row_idx, cls in enumerate([0, 1]):\n",
        "    for col_idx, sample_idx in enumerate(class_indices[cls]):\n",
        "        # get explanation\n",
        "        explanation = explainer.explain_instance(\n",
        "            X_test[sample_idx].squeeze(),\n",
        "            lime_predict_fn,\n",
        "            top_labels=2,\n",
        "            hide_color=0,\n",
        "            num_samples=500,\n",
        "            batch_size=20\n",
        "        )\n",
        "\n",
        "        # get image and mask\n",
        "        temp, mask = explanation.get_image_and_mask(\n",
        "            explanation.top_labels[0],\n",
        "            positive_only=True,\n",
        "            num_features=5,\n",
        "            hide_rest=False\n",
        "        )\n",
        "\n",
        "        # prediction\n",
        "        pred_probs = lime_predict_fn(X_test[sample_idx].reshape(1, 128, 128, 1))[0]\n",
        "\n",
        "        # plot\n",
        "        ax = plt.subplot2grid((2, 3), (row_idx, col_idx))\n",
        "        ax.imshow(temp, cmap='gray')\n",
        "        ax.imshow(mask, cmap='jet', alpha=0.4)\n",
        "        ax.set_title(\n",
        "            f\"True: {class_names[cls]}\\n\"\n",
        "            f\"Pred: {class_names[np.argmax(pred_probs)]} \"\n",
        "            f\"({np.max(pred_probs):.2f})\",\n",
        "            fontsize=8\n",
        "        )\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eeKQXWK7UAEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LIME (XAI Discussion)\n",
        "\n",
        "Our LIME results reveal important insights into the interpretability and limitations of our model's decision-making process. In non-lesion images, LIME successfully highlights super-pixels within the anatomical regions, such as the liver, kidneys, and surrounding soft tissues, where lesions are expected to happen. These regions are marked in red, which indicate a strong contribution to the model's \"positive\" classification. This alignment between the highlighted regions and the actual lesion images suggests that, for these specific cases, the model has learned to associate relevant internal organ features, which is a promising sign of pattern recognition and that the model is functioning as we intended.\n",
        "\n",
        "In contrast, the LIME maps for lesion images display red activation zones primarily outside the organ boundaries, often in the background or along image edges. This is concerning, as it implies that our model is relying on irrelevant features for classification. This behavior raises the possibility of dataset bias, such as differences in image padding, data source inconsistencies, or unsuitable preprocessing between lesion and non-lesion samples. As LIME explanations are local, and change the image using super-pixel modifications, the fact that these external regions are repeatedly highlighted as important indicates that the model might not have a comprehensive, diverse representation of what a healthy abdomen \"should\" look like. Instead, it may be exploiting shortcuts or unintended correlations in the training data (most likely case).\n",
        "\n",
        "These findings reinforce known limitations of LIME:\n",
        "*   Highly dependent on local sampling, and the explanations are sensitive to how features (super-pixels) are defined and modified.\n",
        "*   Since LIME assumes that the model's decision boundary can be approximated locally with a simpler model, its effectiveness diminishes in high-dimensional spaces like medical imaging and CT scans, where tiny pixels often carry critical meaning.\n",
        "*   Additionally, if the feature space that is generated through super-pixel segments does not reflect understandable regions, the results may be misleading."
      ],
      "metadata": {
        "id": "yIib5QMb1aHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Performance and XAI Discussion"
      ],
      "metadata": {
        "id": "xIbxK_ttLln5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Glassbox vs. Blackbox discussion\n",
        "\n",
        "The choice to employ a blackbox CNN for abdominal lesion classification came from CNNs' proven performance (and our previous experience) in capturing layered spatial patterns found in medical imaging, from local texture changes to full anatomical structures. While this architecture sacrifices the inherent transparency of glassbox models, like a decision tree for example, it does achieve relatively good accuracy for clinical relevance. To bridge this interpretability and XAI gap, our project implemented Saliency Maps and LIME, creating an approach that balances model performance with explainable AI. Saliency Maps provided pixel-level visualizations through gradient backpropagation and overlaying, which explain to users via highlighting, that specific regions like lesion margins or calcification patterns are the reason why the model made a specific decision. LIME complements this by generating local explanations through input adjustments, revealing how combinations of pixels generate specific predictions.\n",
        "\n",
        "This double XAI strategy addresses key requirements: Saliency Maps offer real-time verification during model development that the CNN focuses on plausible areas of the image, while LIME supports case-by-case analysis, which is important for identifying when anomalies or rare variants mislead the model. The tradeoffs are as follows:\n",
        "*   Saliency Maps computational efficiency (by using a single backward pass) enables routine use, while LIME's higher resource costs (1000+ perturbations per image) are reserved for uncertain predictions. Ethically, these tools shift the system from blackbox to more of a \"greybox\", allowing clinicians to audit decisions without sacrificing performance.\n",
        "\n",
        "However, we did identify some limitations: Saliency Maps may overemphasize edges (as seen in our images), while LIME's linear approximations occasionally oversimplify the CNN's reasoning (again, shown in our images). Future iterations could integrate adaptive explanation triggers based on prediction confidence, or similar. This approach acknowledges that in medical AI, interpretability and XAI isn't a binary choice, but a spectrum where the optimal balance depends on continuous validation that model explanations align with clinical, human expertise across diverse patient data."
      ],
      "metadata": {
        "id": "vDJxAuPo7m4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "This project explored the development and explainability of a CNN for classifying lesions in abdominal CT scans. We combined datasets from Mendeley and the NIH, preprocessed images, and trained several CNN models with different kernel sizes and batch sizes. Model B emerged as the best-performing model based on accuracy and loss metrics, and further investigation revealed slight differences in performance between transfer learning and a baseline CNN.\n",
        "\n",
        "We utilized XAI techniques like Grad-CAM and LIME to visualize and interpret the model's decision-making process. Saliency maps highlighted the regions of the image that most strongly influenced the classification, promoting trust and transparency. LIME provided local explanations by perturbing the input image and observing the model's response, offering insights into feature importance.\n",
        "\n",
        "Regarding Glassbox vs. Blackbox models, while traditional machine learning models like Logistic Regression and KNN offer inherent interpretability (glassbox), deep learning models like CNNs are often considered black boxes due to their complex architectures. However, XAI techniques help bridge this gap by providing insights into the inner workings of black-box models. By using LIME and Saliency Maps, this project aimed to increase the transparency and trustworthiness of the chosen CNN model. This is crucial, particularly in medical imaging applications where high-stakes decisions require clear justification and interpretability.\n",
        "\n",
        "The use of XAI techniques like Grad-CAM and LIME enhances the transparency and trustworthiness of deep learning models, aligning with the principles of responsible AI in healthcare. Future work can focus on refining the model with larger, more diverse datasets, and exploring advanced XAI methods for more comprehensive model explanations. This would ensure continued progress toward building ethical and reliable AI systems for lesion detection and patient care."
      ],
      "metadata": {
        "id": "k3eOswiCMRa0"
      }
    }
  ]
}